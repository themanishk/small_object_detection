{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  **IMPROVED FASTER RCNN** implemented on code based on faster rcnn by https://github.com/abc99lr/traffic-sign-faster-rcnn"},{"metadata":{"id":"Vvd4_cFsoFtT","outputId":"e6ab05e5-b50b-42fd-af87-4d7720e8c33f","trusted":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport random\nimport pprint\nimport sys\nimport time\nimport numpy as np\nfrom optparse import OptionParser\nimport pickle\nimport math\nimport cv2\nimport copy\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport os\nfrom skimage import io\nfrom sklearn.metrics import average_precision_score\nimport keras\nfrom keras import backend as K\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout,UpSampling2D,Add,Concatenate\nfrom keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\nfrom keras.engine.topology import get_source_inputs\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.objectives import categorical_crossentropy\n\nfrom keras.models import Model\nfrom keras.utils import generic_utils\nfrom keras.engine import Layer, InputSpec\nfrom keras import initializers, regularizers","execution_count":null,"outputs":[]},{"metadata":{"id":"J1rqiEVVKH0v"},"cell_type":"markdown","source":"#config"},{"metadata":{"id":"L7AxhwTpJvM4","trusted":true},"cell_type":"code","source":"class Config:\n\n\tdef __init__(self):\n\n\t\t# Print the process or not\n\t\tself.verbose = True\n\n\t\t# Name of base network\n\t\tself.network = 'vgg'\n\n\t\t# Setting for data augmentation\n\t\tself.use_horizontal_flips = False\n\t\tself.use_vertical_flips = False\n\t\tself.rot_90 = False\n\n\t\t# Anchor box scales\n    # Note that if im_size is smaller, anchor_box_scales should be scaled\n    # Original anchor_box_scales in the paper is [128, 256, 512]\n\t\tself.anchor_box_scales = [64, 128, 256] \n\n\t\t# Anchor box ratios\n\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n\n\t\t# Size to resize the smallest side of the image\n\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n\t\tself.im_size = 448\n\n\t\t# image channel-wise mean to subtract\n\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n\t\tself.img_scaling_factor = 1.0\n\n\t\t# number of ROIs at once\n\t\tself.num_rois = 4\n\n\t\t# stride at the RPN (this depends on the network configuration)\n\t\tself.rpn_stride = 8\n\n\t\tself.balanced_classes = False\n\n\t\t# scaling the stdev\n\t\tself.std_scaling = 4.0\n\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n\n\t\t# overlaps for RPN\n\t\tself.rpn_min_overlap = 0.3\n\t\tself.rpn_max_overlap = 0.7\n\n\t\t# overlaps for classifier ROIs\n\t\tself.classifier_min_overlap = 0.1\n\t\tself.classifier_max_overlap = 0.5\n\n\t\t# placeholder for the class mapping, automatically generated by the parser\n\t\tself.class_mapping = None\n\n\t\tself.model_path = None","execution_count":null,"outputs":[]},{"metadata":{"id":"Bj0LGtYKKM2D"},"cell_type":"markdown","source":"Parser the data from annotation file\n"},{"metadata":{"id":"ZArRGC0-KMOL","trusted":true},"cell_type":"code","source":"def get_data(input_path):\n    found_bg = False\n    all_imgs = {}\n    classes_count = {}\n    class_mapping = {}\n\n    with open(input_path, 'r') as f:\n        print('Parsing annotation files')\n\n        for line in f:\n            line_split = line.strip().split(',')\n            (filename, x1, y1, x2, y2, class_name) = line_split\n\n            if class_name not in classes_count:\n                classes_count[class_name] = 1\n            else:\n                classes_count[class_name] += 1\n\n            if class_name not in class_mapping:\n                if class_name == 'bg' and found_bg is False:\n                    print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n                    found_bg = True\n                class_mapping[class_name] = len(class_mapping)\n\n            if filename not in all_imgs:\n                all_imgs[filename] = {}\n\n                file_path = '../input/improvedv2/improved/' + filename\n                img = io.imread(file_path)\n                (rows, cols) = img.shape[:2]\n                all_imgs[filename]['filepath'] = file_path\n                all_imgs[filename]['width'] = cols\n                all_imgs[filename]['height'] = rows\n                all_imgs[filename]['bboxes'] = []\n\n                # we use all images as training images\n                all_imgs[filename]['imageset'] = 'train'     # modify to split train img for testing\n\n            all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n\n        all_data = []\n        for key in all_imgs:\n            all_data.append(all_imgs[key])\n\n        # make sure the bg class is last in the list\n        if found_bg:\n            if class_mapping['bg'] != len(class_mapping) - 1:\n                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n                val_to_switch = class_mapping['bg']\n                class_mapping['bg'] = len(class_mapping) - 1\n                class_mapping[key_to_switch] = val_to_switch\n\n        return all_data, classes_count, class_mapping","execution_count":null,"outputs":[]},{"metadata":{"id":"CHcD5Gb6Kc0T"},"cell_type":"markdown","source":"# # **Define IMPROVED ROI Pooling Convolutional Layer with bilinear interpolation**"},{"metadata":{"id":"LJv_J7ZlKXPe","trusted":true},"cell_type":"code","source":"class RoiPoolingConv(Layer):\n    \"\"\"\n    ROI pooling layer for 2D inputs.\n    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n    K. He, X. Zhang, S. Ren, J. Sun\n    # Arguments\n        pool_size(int): Size of pooling region to use. pool_size = 7 will result in a 7x7 region\n        num_rois: number of RoI to be used\n    # Input shape\n        list of two 4D tensors [X_img, X_roi] with shape:\n        X_img: (1, rows, cols, channels)\n        X_roi: (1,num_rois,4) list of rois, with ordering (x,y,w,h)\n    # Output shape\n        3D tensor with shape: (1, num_rois, channels, pool_size, pool_size)\n    \"\"\"\n\n    def __init__(self, pool_size, num_rois, **kwargs):\n\n        self.pool_size = pool_size\n        self.num_rois = num_rois\n\n        super(RoiPoolingConv, self).__init__(**kwargs)\n\n\n    def build(self, input_shape):\n        self.nb_channels = input_shape[0][3]\n\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        if self.dim_ordering == 'th':\n            return None, self.num_rois, self.nb_channels, self.pool_size, self.pool_size\n        else:\n            return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n        \"\"\"\n        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n    \n        \n        \n    def call(self, x, mask=None):\n        assert(len(x) == 2)\n        img, rois = x[0], x[1]\n        input_shape = K.shape(img)\n        input_shape = K.cast(input_shape, tf.float32)\n        outputs = []\n        row,col = img.shape[1:3]\n        bboxes=[]\n        for roi_idx in range(self.num_rois):\n            x1 = rois[0, roi_idx, 0]\n            y1 = rois[0, roi_idx, 1]\n            x2 = rois[0, roi_idx, 2] + x1\n            y2 = rois[0, roi_idx, 3] + y1\n            #print(x1,input_shape)\n            x1 = x1 / input_shape[2]\n            y1 = y1 / input_shape[1]\n            x2 = x2 / input_shape[2]\n            y2 = y2 / input_shape[1]\n\n            x1 = K.cast(x1, 'float32')\n            y1 = K.cast(y1, 'float32')\n            x2 = K.cast(x2, 'float32')\n            y2 = K.cast(y2, 'float32')\n            bboxes.append([y1, x1, y2, x2])\n        bboxes = tf.reshape(bboxes,(self.num_rois,4))\n        #bboxes = self.get_bboxes(rois, input_shape)\n        bboxes_shape = tf.shape(bboxes)\n        batch_ids = tf.zeros((bboxes_shape[0], ), dtype=tf.int32)\n        # Apply crop and resize with extracting a crop double the desired size.\n        outputs = tf.image.crop_and_resize(img, bboxes, batch_ids,[self.pool_size, self.pool_size],method='bilinear', name=\"crops\")\n        #rs = tf.image.resize_images(img[:, y:y + h, x:x + w, :], (self.pool_size, self.pool_size))\n        #outputs.append(rs)\n\n        final_output = outputs\n        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n\n        return final_output\n    \n    \n    def get_config(self):\n        config = {'pool_size': self.pool_size, 'num_rois': self.num_rois}\n        base_config = super(RoiPoolingConv, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"id":"1FWVzRo-Kl_d"},"cell_type":"markdown","source":"# # **IMPROVED base net Vgg-16 model******"},{"metadata":{"id":"GiLzeeuKKlUy","trusted":true},"cell_type":"code","source":"def img_length_calc_function(C, width, height):\n    def get_output_length(input_length):\n        return input_length / 8\n    return get_output_length(width), get_output_length(height)  \n\ndef nn_base(input_tensor=None, trainable=False):\n    \"\"\"\n    Based ConvNet shared by both RPN and ROI Pooling layer, implemented by a midified VGG-16,\n    and returns a feature map.\n    C.rpn_stride is set such that it corresponds to this base network\n    \"\"\"\n\n    if input_tensor is None:\n        img_input = Input(shape=(None, None, 3))\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=(None, None, 3))\n        else:\n            img_input = input_tensor\n\n    bn_axis = 3\n    #model=keras.applications.vgg16.VGG16(weights='imagenet')\n    # Block 1\n    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(conv1)\n    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(conv2)\n\n    # Block 2\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(pool1)\n    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(conv3)\n    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(conv4)\n\n    # Block 3\n    conv5 = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(pool2)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(conv5)\n    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(conv6)\n    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(conv7)\n\n    # Block 4\n    conv8 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(pool3)\n    conv9 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(conv8)\n    conv10 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(conv9)\n    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(conv10)\n\n    # Block 5\n    conv11 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(pool4)\n    conv12 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(conv11)\n    conv13 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(conv12)\n    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n    up1 = UpSampling2D(size=(2, 2), data_format=None, interpolation='bilinear')(conv13)\n    \n    lrn1 = pool3 #model.get_layer('block3_pool').output #tf.nn.lrn((pool3))\n    lrn2 =  conv10 #model.get_layer('block4_conv3').output #tf.nn.lrn((conv10))\n    lrn3 = up1 #tf.nn.lrn((up1))\n    \n    lrn1 = Conv2D(1, (1, 1), activation='relu',kernel_initializer='ones', padding='same', name='lrn1')(lrn1)\n    lrn2 = Conv2D(1, (1, 1), activation='relu',kernel_initializer='ones', padding='same', name='lrn2')(lrn2)\n    lrn3 = Conv2D(1, (1, 1), activation='relu',kernel_initializer='ones', padding='same', name='lrn3')(lrn3)\n    x = Add()([lrn1,lrn2,lrn3])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"id":"ev2O8oemMqPQ"},"cell_type":"markdown","source":"RPN layer"},{"metadata":{"id":"LutHJ2Z2KvIP","trusted":true},"cell_type":"code","source":"def rpn_layer(base_layers, num_anchors):\n    \"\"\"Create a rpn layer\n        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n                Keep the padding 'same' to preserve the feature map's size\n        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n    Args:\n        base_layers: vgg in here\n        num_anchors: 9 in here\n\n    Returns:\n        [x_class, x_regr, base_layers]\n        x_class: classification for whether it's an object\n        x_regr: bboxes regression\n        base_layers: vgg in here\n    \"\"\"\n    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n\n    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n\n    return [x_class, x_regr, base_layers]","execution_count":null,"outputs":[]},{"metadata":{"id":"1XFLW0W7Owpm"},"cell_type":"markdown","source":"Classifier layer"},{"metadata":{"id":"WLj6gfP6MydC","trusted":true},"cell_type":"code","source":"def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n    \"\"\"Create a classifier layer\n    \n    Args:\n        base_layers: vgg\n        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n        num_rois: number of rois to be processed in one time (4 in here)\n\n    Returns:\n        list(out_class, out_regr)\n        out_class: classifier layer output\n        out_regr: regression layer output\n    \"\"\"\n\n    input_shape = (num_rois,7,7,512)\n\n    pooling_regions = 7\n\n    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n    # num_rois (4) 7x7 roi pooling\n    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n\n    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n    out = TimeDistributed(Dropout(0.5))(out)\n    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n    out = TimeDistributed(Dropout(0.5))(out)\n\n    # There are two output layer\n    # out_class: softmax acivation function for classify the class name of the object\n    # out_regr: linear activation function for bboxes coordinates regression\n    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n    # note: no regression target for bg class\n    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n\n    return [out_class, out_regr]","execution_count":null,"outputs":[]},{"metadata":{"id":"uB_p3Nu2O4oP"},"cell_type":"markdown","source":"Calculate IoU (Intersection of Union)"},{"metadata":{"id":"dn_dBNtCO38-","trusted":true},"cell_type":"code","source":"def union(au, bu, area_intersection):\n\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n\tarea_union = area_a + area_b - area_intersection\n\treturn area_union\n\n\ndef intersection(ai, bi):\n\tx = max(ai[0], bi[0])\n\ty = max(ai[1], bi[1])\n\tw = min(ai[2], bi[2]) - x\n\th = min(ai[3], bi[3]) - y\n\tif w < 0 or h < 0:\n\t\treturn 0\n\treturn w*h\n\n\ndef iou(a, b):\n\t# a and b should be (x1,y1,x2,y2)\n\n\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n\t\treturn 0.0\n\n\tarea_i = intersection(a, b)\n\tarea_u = union(a, b, area_i)\n\n\treturn float(area_i) / float(area_u + 1e-6)","execution_count":null,"outputs":[]},{"metadata":{"id":"JHoYow4sPXPb"},"cell_type":"markdown","source":"\nCalculate the rpn for all anchors of all images\n"},{"metadata":{"id":"wi8b8lQRPC_E","trusted":true},"cell_type":"code","source":"def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n\n    downscale = float(8)\n    anchor_sizes = [64, 128, 256] \n    anchor_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n    num_anchors = len(anchor_sizes) * len(anchor_ratios)\n    n_anchratios = len(anchor_ratios)\n    # calculate the output map size based on the network architecture\n    (output_width, output_height) = img_length_calc_function(C, resized_width, resized_height)\n    output_width, output_height = int(output_width), int(output_height)\n\n    # initialise empty output objectives\n    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n\n    num_bboxes = len(img_data['bboxes'])\n    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n    best_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n\n    # get the GT box coordinates, and resize to account for image resizing\n    gta = np.zeros((num_bboxes, 4))\n    for bbox_num, bbox in enumerate(img_data['bboxes']):\n        # get the GT box coordinates, and resize to account for image resizing\n        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n\n    # rpn ground truth\n    for anchor_size_idx in range(len(anchor_sizes)):\n        for anchor_ratio_idx in range(n_anchratios):\n            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n\n            for ix in range(output_width):\n                # x-coordinates of the current anchor box\n                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\n\n                # ignore boxes that go across image boundaries\n                if x1_anc < 0 or x2_anc > resized_width:\n                    continue\n\n                for jy in range(output_height):\n                    # y-coordinates of the current anchor box\n                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n\n                    # ignore boxes that go across image boundaries\n                    if y1_anc < 0 or y2_anc > resized_height:\n                        continue\n\n                    # bbox_type indicates whether an anchor should be a target\n                    bbox_type = 'neg'\n\n                    # this is the best IOU for the (x,y) coord and the current anchor\n                    # note that this is different from the best IOU for a GT bbox\n                    best_iou_for_loc = 0.0\n\n                    for bbox_num in range(num_bboxes):\n\n                        # get IOU of the current GT box and the current anchor box\n                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n                        # calculate the regression targets if they will be needed\n                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > 0.7:\n                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n                            cxa = (x1_anc + x2_anc)/2.0\n                            cya = (y1_anc + y2_anc)/2.0\n\n                            tx = (cx - cxa) / (x2_anc - x1_anc)\n                            ty = (cy - cya) / (y2_anc - y1_anc)\n                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n\n                        if img_data['bboxes'][bbox_num]['class'] != 'bg':\n\n                            # all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n                            if curr_iou > best_iou_for_bbox[bbox_num]:\n                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n                                best_iou_for_bbox[bbox_num] = curr_iou\n                                best_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n                                best_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n\n                            # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n                            if curr_iou > 0.7:\n                                bbox_type = 'pos'\n                                num_anchors_for_bbox[bbox_num] += 1\n                                # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n                                if curr_iou > best_iou_for_loc:\n                                    best_iou_for_loc = curr_iou\n                                    best_regr = (tx, ty, tw, th)\n\n                            # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n                            if 0.3 < curr_iou < 0.7:\n                                # gray zone between neg and pos\n                                if bbox_type != 'pos':\n                                    bbox_type = 'neutral'\n\n                    # turn on or off outputs depending on IOUs\n                    if bbox_type == 'neg':\n                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n                    elif bbox_type == 'neutral':\n                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n                    elif bbox_type == 'pos':\n                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n                        start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n                        y_rpn_regr[jy, ix, start:start+4] = best_regr\n\n    # we ensure that every bbox has at least one positive RPN region\n    for idx in range(num_anchors_for_bbox.shape[0]):\n        if num_anchors_for_bbox[idx] == 0:\n            # no box with an IOU greater than zero ...\n            if best_anchor_for_bbox[idx, 0] == -1:\n                continue\n            y_is_box_valid[\n                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n                best_anchor_for_bbox[idx,3]] = 1\n            y_rpn_overlap[\n                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n                best_anchor_for_bbox[idx,3]] = 1\n            start = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n            y_rpn_regr[\n                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n\n    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n\n    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n\n    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n\n    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n\n    num_pos = len(pos_locs[0])\n\n    # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n    # regions. We also limit it to 256 regions.\n    num_regions = 256\n\n    if len(pos_locs[0]) > num_regions/2:\n        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n        num_pos = num_regions/2\n\n    if len(neg_locs[0]) + num_pos > num_regions:\n        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n\n    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n\n    return np.copy(y_rpn_cls), np.copy(y_rpn_regr)","execution_count":null,"outputs":[]},{"metadata":{"id":"1qn0nd2RPllj"},"cell_type":"markdown","source":"Get new image size and augment the image\n"},{"metadata":{"id":"iUHFPELdPk5D","trusted":true},"cell_type":"code","source":"def get_new_img_size(width, height, img_min_side=448):\n\tif width <= height:\n\t\tf = float(img_min_side) / width\n\t\tresized_height = int(f * height)\n\t\tresized_width = img_min_side\n\telse:\n\t\tf = float(img_min_side) / height\n\t\tresized_width = int(f * width)\n\t\tresized_height = img_min_side\n\n\treturn resized_width, resized_height\n\ndef augment(img_data, config, augment=True):\n\tassert 'filepath' in img_data\n\tassert 'bboxes' in img_data\n\tassert 'width' in img_data\n\tassert 'height' in img_data\n\n\timg_data_aug = copy.deepcopy(img_data)\n\n\timg = cv2.imread(img_data_aug['filepath'])\n\n\tif augment:\n\t\trows, cols = img.shape[:2]\n\n\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n\t\t\timg = cv2.flip(img, 1)\n\t\t\tfor bbox in img_data_aug['bboxes']:\n\t\t\t\tx1 = bbox['x1']\n\t\t\t\tx2 = bbox['x2']\n\t\t\t\tbbox['x2'] = cols - x1\n\t\t\t\tbbox['x1'] = cols - x2\n\n\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n\t\t\timg = cv2.flip(img, 0)\n\t\t\tfor bbox in img_data_aug['bboxes']:\n\t\t\t\ty1 = bbox['y1']\n\t\t\t\ty2 = bbox['y2']\n\t\t\t\tbbox['y2'] = rows - y1\n\t\t\t\tbbox['y1'] = rows - y2\n\n\t\tif config.rot_90:\n\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n\t\t\tif angle == 270:\n\t\t\t\timg = np.transpose(img, (1,0,2))\n\t\t\t\timg = cv2.flip(img, 0)\n\t\t\telif angle == 180:\n\t\t\t\timg = cv2.flip(img, -1)\n\t\t\telif angle == 90:\n\t\t\t\timg = np.transpose(img, (1,0,2))\n\t\t\t\timg = cv2.flip(img, 1)\n\t\t\telif angle == 0:\n\t\t\t\tpass\n\n\t\t\tfor bbox in img_data_aug['bboxes']:\n\t\t\t\tx1 = bbox['x1']\n\t\t\t\tx2 = bbox['x2']\n\t\t\t\ty1 = bbox['y1']\n\t\t\t\ty2 = bbox['y2']\n\t\t\t\tif angle == 270:\n\t\t\t\t\tbbox['x1'] = y1\n\t\t\t\t\tbbox['x2'] = y2\n\t\t\t\t\tbbox['y1'] = cols - x2\n\t\t\t\t\tbbox['y2'] = cols - x1\n\t\t\t\telif angle == 180:\n\t\t\t\t\tbbox['x2'] = cols - x1\n\t\t\t\t\tbbox['x1'] = cols - x2\n\t\t\t\t\tbbox['y2'] = rows - y1\n\t\t\t\t\tbbox['y1'] = rows - y2\n\t\t\t\telif angle == 90:\n\t\t\t\t\tbbox['x1'] = rows - y2\n\t\t\t\t\tbbox['x2'] = rows - y1\n\t\t\t\t\tbbox['y1'] = x1\n\t\t\t\t\tbbox['y2'] = x2        \n\t\t\t\telif angle == 0:\n\t\t\t\t\tpass\n\n\timg_data_aug['width'] = img.shape[1]\n\timg_data_aug['height'] = img.shape[0]\n\treturn img_data_aug, img","execution_count":null,"outputs":[]},{"metadata":{"id":"e7bJ6bU8P5_8"},"cell_type":"markdown","source":"Generate the ground_truth anchors\n"},{"metadata":{"id":"LvE2hPCHPxWh","trusted":true},"cell_type":"code","source":"def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n    \"\"\"\n    Find ground-truth anchors:\n    gt positive anchors: overlap with ground-truth bbox above threshold rpn_max_overlap\n    gt negative anchors: overlap with ground-truth bbox below threshold rpn_min_overlap\n    do not train anchors with overlap in between\n    \"\"\"\n    #sample_selector = SampleSelector(class_count)\n\n    while True:\n        if mode == 'train':\n            np.random.shuffle(all_img_data)\n\n        for img_data in all_img_data:\n            try:\n                #if C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):\n                #continue\n                # read in image, and optionally add augmentation\n                if mode == 'train':\n                    img_data_aug, x_img = augment(img_data, C, augment=False)\n                else:\n                    img_data_aug, x_img = augment(img_data, C, augment=False)\n                ''' img_data = {'filepath': '../dataset/PNG_train/00269.png', 'width': 1360, \n                'height': 800, 'bboxes': [{'class': '9', 'x1': 966, 'x2': 990, 'y1': 430, \n                'y2': 454}], 'imageset': 'trainval'} '''\n\n                ''' img_data_aug = {'filepath': '../dataset/PNG_train/00269.png', 'width': 1360, \n                'height': 800, 'bboxes': [{'class': '9', 'x1': 966, 'x2': 990, 'y1': 430, \n                'y2': 454}], 'imageset': 'trainval'} '''\n\n                ''' x_img = RGB image (800, 1360, 3)'''\n\n                (width, height) = (img_data_aug['width'], img_data_aug['height'])\n                (rows, cols, _) = x_img.shape\n                assert cols == width\n                assert rows == height\n\n                # get image dimensions for resizing\n                (resized_width, resized_height) = (448,448)\n                # resize the image so that smalles side is length = 600 pixel\n                x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n                if 1 :\n                    y_rpn_cls, y_rpn_regr = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n                else:\n                    continue\n\n                # Zero-center by mean pixel, and preprocess image\n                x_img = x_img[:, :, (2, 1, 0)]  # BGR -> RGB\n                x_img = x_img.astype(np.float32)\n                x_img[:, :, 0] -= 103.939\n                x_img[:, :, 1] -= 116.779\n                x_img[:, :, 2] -= 123.68\n                x_img /= 1.0\n\n                x_img = np.transpose(x_img, (2, 0, 1))\n                x_img = np.expand_dims(x_img, axis=0)\n\n                y_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= 4\n\n                x_img = np.transpose(x_img, (0, 2, 3, 1))\n                y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n                y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n                yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug\n\n            except Exception as e:\n                print(e)\n                continue","execution_count":null,"outputs":[]},{"metadata":{"id":"8FlcNe-cQAID"},"cell_type":"markdown","source":"# # **IMPROVED loss functions for regressions based on improved iou **\n"},{"metadata":{"id":"GJKLxDNZP-4M","trusted":true},"cell_type":"code","source":"lambda_rpn_regr = 1.0\nlambda_rpn_class = 1.0\n\nlambda_cls_regr = 1.0\nlambda_cls_class = 1.0\n\nepsilon = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rpn_loss_regr(num_anchors):\n    \"\"\"Loss function for rpn regression\n    Args:\n        num_anchors: number of anchors (9 in here)\n    Returns:\n        Smooth L1 loss function \n                           0.5*x*x (if x_abs < 1)\n                           x_abx - 0.5 (otherwise)\n    \"\"\"\n    def rpn_loss_regr_fixed_num(y_true, y_pred):\n        '''x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n\n        # absolute value of x\n        x_abs = K.abs(x)\n\n        # If x_abs <= 1.0, x_bool = 1\n        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n        return lambda_rpn_regr * K.sum(y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])'''\n\n        total_reg_loss = 0\n        d = y_true[:, :, :, 4 * num_anchors:] *y_true[:, :, :, :4*num_anchors]\n        y_pred = y_pred *y_true[:, :, :, :4 * num_anchors]\n        N_reg = K.cast(K.sum(1e-5 + y_true[:, :, :, :4*num_anchors])/4,'float64')\n        for i in range(num_anchors):\n            a = y_pred[:,:,:,4*i:4*(i+1)]\n            b = d[:,:,:,4*i:4*(i+1)]\n            s = K.cast(abs((a[:,:,:,2]-a[:,:,:,0])*(a[:,:,:,3]-a[:,:,:,1])),'float64')*K.cast(K.greater((a[:,:,:,2]-a[:,:,:,0]),0),'float64')*K.cast(K.greater(a[:,:,:,3]-a[:,:,:,1],0),'float64')\n            s1 = K.cast(abs((b[:,:,:,2]-b[:,:,:,0])*(b[:,:,:,3]-b[:,:,:,1])),'float64')*K.cast(K.greater((b[:,:,:,2]-b[:,:,:,0]),0),'float64')*K.cast(K.greater(b[:,:,:,3]-b[:,:,:,1],0),'float64')\n            x1 = K.maximum(a[:,:,:,0], b[:,:,:,0])\n            y1 = K.maximum(a[:,:,:,1], b[:,:,:,1])\n            x2 = K.minimum(a[:,:,:,2], b[:,:,:,2])\n            y2 = K.minimum(a[:,:,:,3], b[:,:,:,3])\n            si = K.cast(abs((x2-x1)*(y2-y1)),'float64')*K.cast(K.greater((x2-x1),0),'float64')*K.cast(K.greater((y2-y1),0),'float64')\n            #print(K.cast((x2-x1),'float32')*K.cast(K.greater((x2-x1),0),'float32'))\n            x1c = K.minimum(a[:,:,:,0], b[:,:,:,0])\n            y1c = K.minimum(a[:,:,:,1], b[:,:,:,1])\n            x2c = K.maximum(a[:,:,:,2], b[:,:,:,2])\n            y2c = K.maximum(a[:,:,:,3], b[:,:,:,3])\n            sc = K.cast(abs((x2c-x1c)*(y2c-y1c)),'float64')*K.cast(K.greater((x2c-x1c),0),'float64')*K.cast(K.greater((y2c-y1c),0),'float64')\n            #iiou = (si/(s+s1-si+(1e-5)) - (sc - (s+s1-si))/sc)\n            iiou = tf.math.divide_no_nan(si,s+s1-si) - tf.math.divide_no_nan(sc-s-s1+si,sc)\n            l = 1 - iiou\n            l = l*K.cast(y_true[:, :, :,4*i],'float64')\n            total_reg_loss += K.sum(l)\n        rpn_reg_loss = K.cast((total_reg_loss/N_reg),'float32')\n        return rpn_reg_loss\n\n    return rpn_loss_regr_fixed_num\n\n\ndef rpn_loss_cls(num_anchors):\n    \"\"\"Loss function for rpn classification\n    Args:\n        num_anchors: number of anchors (9 in here)\n        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n    Returns:\n        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n    \"\"\"\n    def rpn_loss_cls_fixed_num(y_true, y_pred):\n            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n\n    return rpn_loss_cls_fixed_num\n\n\ndef class_loss_regr(num_classes):\n    \"\"\"Loss function for rpn regression\n    Args:\n        num_anchors: number of anchors (9 in here)\n    Returns:\n        Smooth L1 loss function \n                           0.5*x*x (if x_abs < 1)\n                           x_abx - 0.5 (otherwise)\n    \"\"\"\n    def class_loss_regr_fixed_num(y_true, y_pred):\n        '''d = y_true[:, :, 4*num_classes:] - y_pred\n        p_star = K.cast(K.less_equal(K.abs(d), 1.0), tf.float32)\n        N_reg = K.sum(1e-5 + y_true[:, :, :4*num_classes])\n\n        l1_smooth = 0.5 * d * d if p_star == 1 else K.abs(d) - 0.5\n        total_reg_loss = K.sum(y_true[:, :, :4*num_classes] * l1_smooth)\n        class_reg_loss = total_reg_loss / N_reg'''\n        total_reg_loss = 0\n        d = y_true[:, :, 4 * num_classes:] #*y_true[ :, :, :4*num_classes]\n        y_pred = y_pred #*y_true[ :, :, :4 * num_classes]\n        N_reg = K.cast(K.sum(1e-5 + y_true[ :, :, :4*num_classes])/4,'float64')\n        for i in range(num_classes):\n            a = y_pred[:,:,4*i:4*(i+1)]\n            b = d[:,:,4*i:4*(i+1)]\n            s = K.cast(abs((a[:,:,2]-a[:,:,0])*(a[:,:,3]-a[:,:,1])),'float64')*K.cast(K.greater((a[:,:,2]-a[:,:,0]),0),'float64')*K.cast(K.greater(a[:,:,3]-a[:,:,1],0),'float64')\n            s1 = K.cast(abs((b[:,:,2]-b[:,:,0])*(b[:,:,3]-b[:,:,1])),'float64')*K.cast(K.greater((b[:,:,2]-b[:,:,0]),0),'float64')*K.cast(K.greater(b[:,:,3]-b[:,:,1],0),'float64')\n            x1 = K.maximum(a[:,:,0], b[:,:,0])\n            y1 = K.maximum(a[:,:,1], b[:,:,1])\n            x2 = K.minimum(a[:,:,2], b[:,:,2])\n            y2 = K.minimum(a[:,:,3], b[:,:,3])\n            si = K.cast(abs((x2-x1)*(y2-y1)),'float64')*K.cast(K.greater((x2-x1),0),'float64')*K.cast(K.greater((y2-y1),0),'float64')\n            x1c = K.minimum(a[:,:,0], b[:,:,0])\n            y1c = K.minimum(a[:,:,1], b[:,:,1])\n            x2c = K.maximum(a[:,:,2], b[:,:,2])\n            y2c = K.maximum(a[:,:,3], b[:,:,3])\n            sc = K.cast(abs((x2c-x1c)*(y2c-y1c)),'float64')*K.cast(K.greater((x2c-x1c),0),'float64')*K.cast(K.greater((y2c-y1c),0),'float64')\n            #iiou = (si/(s+s1-si+(1e-5)) - (sc - (s+s1-si))/sc)\n            iiou = tf.math.divide_no_nan(si,s+s1-si) - tf.math.divide_no_nan(sc-s-s1+si,sc)\n            l = 1 - iiou\n            l = l*K.cast(y_true[ :, :,4*i],'float64')\n            total_reg_loss += K.sum(l)\n        class_reg_loss = K.cast(total_reg_loss /N_reg,'float32')\n        return class_reg_loss\n    \n    return class_loss_regr_fixed_num\n\n\ndef class_loss_cls(y_true, y_pred):\n    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def rpn_loss_regr(num_anchors):\n    \"\"\"Loss function for rpn regression\n    Args:\n        num_anchors: number of anchors (9 in here)\n    Returns:\n        Smooth L1 loss function \n                           0.5*x*x (if x_abs < 1)\n                           x_abx - 0.5 (otherwise)\n    \"\"\"\n    def rpn_loss_regr_fixed_num(y_true, y_pred):\n        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n\n        # absolute value of x\n        x_abs = K.abs(x)\n\n        # If x_abs <= 1.0, x_bool = 1\n        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n        return lambda_rpn_regr * K.sum(y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n\n\n    return rpn_loss_regr_fixed_num\n\n\ndef rpn_loss_cls(num_anchors):\n    \"\"\"Loss function for rpn classification\n    Args:\n        num_anchors: number of anchors (9 in here)\n        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n    Returns:\n        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n    \"\"\"\n    def rpn_loss_cls_fixed_num(y_true, y_pred):\n            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n\n    return rpn_loss_cls_fixed_num\n\n\ndef class_loss_regr(num_classes):\n    \"\"\"Loss function for rpn regression\n    Args:\n        num_anchors: number of anchors (9 in here)\n    Returns:\n        Smooth L1 loss function \n                           0.5*x*x (if x_abs < 1)\n                           x_abx - 0.5 (otherwise)\n    \"\"\"\n    def class_loss_regr_fixed_num(y_true, y_pred):\n        x = y_true[:, :, 4*num_classes:] - y_pred\n        x_abs = K.abs(x)\n        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n    return class_loss_regr_fixed_num\n\ndef class_loss_cls(y_true, y_pred):\n    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))"},{"metadata":{"id":"ADQUF45AQLZx"},"cell_type":"markdown","source":"# # **improved soft non-max-supperssion**"},{"metadata":{"id":"DbI2W714QKZ5","trusted":true},"cell_type":"code","source":"def improved_non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    np.testing.assert_array_less(x1, x2)\n    np.testing.assert_array_less(y1, y2)\n\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    # initialize the list of picked indexes\n    pick = []\n\n    # calculate the areas\n    area = (x2 - x1) * (y2 - y1)\n\n    # sort the bounding boxes\n    idxs = len(probs)\n    idx = np.argsort(probs)\n    ind = np.argmax(probs)\n\n    # keep looping while some indexes still remain in the indexes list\n    while len(idx) > 0:\n        # grab the last index in the indexes list and add the\n        # index value to the list of picked indexes\n        last = len(idx) -1\n        ind = np.argmax(probs)\n        i = idx[last]\n        pick.append(i)\n        \n\n        # find the intersection\n\n        xx1_int = np.maximum(x1[i], x1[idx])\n        yy1_int = np.maximum(y1[i], y1[idx])\n        xx2_int = np.minimum(x2[i], x2[idx])\n        yy2_int = np.minimum(y2[i], y2[idx])\n\n        ww_int = np.maximum(0, xx2_int - xx1_int)\n        hh_int = np.maximum(0, yy2_int - yy1_int)\n\n        area_int = ww_int * hh_int\n\n        # find the union\n        area_union = area[i] + area[idx] - area_int\n\n        # compute the ratio of overlap\n        overlap = area_int/(area_union + 1e-6)\n\n        # delete all indexes from the index list that have\n        idxs -= 1\n        probsind = np.where(overlap > overlap_thresh)[0]\n        probs[probsind] = probs[probsind]*(1-overlap[probsind])\n        idx = np.delete(idx, np.concatenate(([last],\n            np.where(overlap > overlap_thresh)[0])))\n\n        if len(pick) >= max_boxes:\n            break\n\n    # return only the bounding boxes that were picked using the integer data type\n    boxes = boxes[pick].astype(\"int\")\n    probs = probs[pick]\n    return boxes, probs\n\n\n\n\ndef apply_regr_np(X, T):\n    \"\"\"Apply regression layer to all anchors in one feature map\n\n    Args:\n        X: shape=(4, 18, 25) the current anchor type for all points in the feature map\n        T: regression layer shape=(4, 18, 25)\n\n    Returns:\n        X: regressed position and size for current anchor\n    \"\"\"\n    try:\n        x = X[0, :, :]\n        y = X[1, :, :]\n        w = X[2, :, :]\n        h = X[3, :, :]\n\n        tx = T[0, :, :]\n        ty = T[1, :, :]\n        tw = T[2, :, :]\n        th = T[3, :, :]\n\n        cx = x + w/2.\n        cy = y + h/2.\n        cx1 = tx * w + cx\n        cy1 = ty * h + cy\n\n        w1 = np.exp(tw.astype(np.float64)) * w\n        h1 = np.exp(th.astype(np.float64)) * h\n        x1 = cx1 - w1/2.\n        y1 = cy1 - h1/2.\n\n        x1 = np.round(x1)\n        y1 = np.round(y1)\n        w1 = np.round(w1)\n        h1 = np.round(h1)\n        return np.stack([x1, y1, w1, h1])\n    except Exception as e:\n        print(e)\n        return X\n    \ndef apply_regr(x, y, w, h, tx, ty, tw, th):\n    # Apply regression to x, y, w and h\n    try:\n        cx = x + w/2.\n        cy = y + h/2.\n        cx1 = tx * w + cx\n        cy1 = ty * h + cy\n        w1 = math.exp(tw) * w\n        h1 = math.exp(th) * h\n        x1 = cx1 - w1/2.\n        y1 = cy1 - h1/2.\n        x1 = int(round(x1))\n        y1 = int(round(y1))\n        w1 = int(round(w1))\n        h1 = int(round(h1))\n\n        return x1, y1, w1, h1\n\n    except ValueError:\n        return x, y, w, h\n    except OverflowError:\n        return x, y, w, h\n    except Exception as e:\n        print(e)\n        return x, y, w, h\n\ndef calc_iou(R, img_data, C, class_mapping):\n    \"\"\"Converts from (x1,y1,x2,y2) to (x,y,w,h) format\n\n    Args:\n        R: bboxes, probs\n    \"\"\"\n    bboxes = img_data['bboxes']\n    (width, height) = (img_data['width'], img_data['height'])\n    # get image dimensions for resizing\n    (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n\n    gta = np.zeros((len(bboxes), 4))\n\n    for bbox_num, bbox in enumerate(bboxes):\n        # get the GT box coordinates, and resize to account for image resizing\n        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)\n        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n        gta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n        gta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n\n    x_roi = []\n    y_class_num = []\n    y_class_regr_coords = []\n    y_class_regr_label = []\n    IoUs = [] # for debugging only\n\n    # R.shape[0]: number of bboxes (=300 from non_max_suppression)\n    for ix in range(R.shape[0]):\n        (x1, y1, x2, y2) = R[ix, :]\n        x1 = int(round(x1))\n        y1 = int(round(y1))\n        x2 = int(round(x2))\n        y2 = int(round(y2))\n\n        best_iou = 0.0\n        best_bbox = -1\n        # Iterate through all the ground-truth bboxes to calculate the iou\n        for bbox_num in range(len(bboxes)):\n            curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n\n            # Find out the corresponding ground-truth bbox_num with larget iou\n            if curr_iou > best_iou:\n                best_iou = curr_iou\n                best_bbox = bbox_num\n\n        if best_iou < C.classifier_min_overlap:\n                continue\n        else:\n            w = x2 - x1\n            h = y2 - y1\n            x_roi.append([x1, y1, w, h])\n            IoUs.append(best_iou)\n\n            if C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n                # hard negative example\n                cls_name = 'bg'\n            elif C.classifier_max_overlap <= best_iou:\n                cls_name = bboxes[best_bbox]['class']\n                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n\n                cx = x1 + w / 2.0\n                cy = y1 + h / 2.0\n\n                tx = (cxg - cx) / float(w)\n                ty = (cyg - cy) / float(h)\n                tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n                th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n            else:\n                print('roi = {}'.format(best_iou))\n                raise RuntimeError\n\n        class_num = class_mapping[cls_name]\n        class_label = len(class_mapping) * [0]\n        class_label[class_num] = 1\n        y_class_num.append(copy.deepcopy(class_label))\n        coords = [0] * 4 * (len(class_mapping) - 1)\n        labels = [0] * 4 * (len(class_mapping) - 1)\n        if cls_name != 'bg':\n            label_pos = 4 * class_num\n            sx, sy, sw, sh = C.classifier_regr_std\n            coords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n            labels[label_pos:4+label_pos] = [1, 1, 1, 1]\n            y_class_regr_coords.append(copy.deepcopy(coords))\n            y_class_regr_label.append(copy.deepcopy(labels))\n        else:\n            y_class_regr_coords.append(copy.deepcopy(coords))\n            y_class_regr_label.append(copy.deepcopy(labels))\n\n    if len(x_roi) == 0:\n        return None, None, None, None\n\n    # bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n    X = np.array(x_roi)\n    # one hot code for bboxes from above => x_roi (X)\n    Y1 = np.array(y_class_num)\n    # corresponding labels and corresponding gt bboxes\n    Y2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n\n    return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs","execution_count":null,"outputs":[]},{"metadata":{"id":"RzdjjEFGQXnz","trusted":true},"cell_type":"code","source":"def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n\t\"\"\"Convert rpn layer to roi bboxes\n\n\tArgs: (num_anchors = 9)\n\t\trpn_layer: output layer for rpn classification \n\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n\t\t\tMight be (1, 18, 25, 18) if resized image is 400 width and 300\n\t\tregr_layer: output layer for rpn regression\n\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n\t\t\tMight be (1, 18, 25, 72) if resized image is 400 width and 300\n\t\tC: config\n\t\tuse_regr: Wether to use bboxes regression in rpn\n\t\tmax_boxes: max bboxes number for non-max-suppression (NMS)\n\t\toverlap_thresh: If iou in NMS is larger than this threshold, drop the box\n\n\tReturns:\n\t\tresult: boxes from non-max-suppression (shape=(300, 4))\n\t\t\tboxes: coordinates for bboxes (on the feature map)\n\t\"\"\"\n\tregr_layer = regr_layer / C.std_scaling\n\n\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n\n\tassert rpn_layer.shape[0] == 1\n\n\t(rows, cols) = rpn_layer.shape[1:3]\n\n\tcurr_layer = 0\n\n\t# A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n\t# Might be (4, 18, 25, 18) if resized image is 400 width and 300\n\t# A is the coordinates for 9 anchors for every point in the feature map \n\t# => all 18x25x9=4050 anchors cooridnates\n\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n\n\tfor anchor_size in anchor_sizes:\n\t\tfor anchor_ratio in anchor_ratios:\n\t\t\t# anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n\t\t\t# anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n\t\t\t\n\t\t\t# curr_layer: 0~8 (9 anchors)\n\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n\n\t\t\t# Create 18x25 mesh grid\n\t\t\t# For every point in x, there are all the y points and vice versa\n\t\t\t# X.shape = (18, 25)\n\t\t\t# Y.shape = (18, 25)\n\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n\n\t\t\t# Calculate anchor position and size for each feature map point\n\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n\n\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n\t\t\tif use_regr:\n\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n\n\t\t\t# Avoid width and height exceeding 1\n\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n\n\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n\t\t\t# x1, y1 is top left coordinate\n\t\t\t# x2, y2 is bottom right coordinate\n\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n\n\t\t\t# Avoid bboxes drawn outside the feature map\n\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n\n\t\t\tcurr_layer += 1\n\n\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n\n\tx1 = all_boxes[:, 0]\n\ty1 = all_boxes[:, 1]\n\tx2 = all_boxes[:, 2]\n\ty2 = all_boxes[:, 3]\n\n\t# Find out the bboxes which is illegal and delete them from bboxes list\n\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n\n\tall_boxes = np.delete(all_boxes, idxs, 0)\n\tall_probs = np.delete(all_probs, idxs, 0)\n\n\t# Apply non_max_suppression\n\t# Only extract the bboxes. Don't need rpn probs in the later process\n\tresult = improved_non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n\n\treturn result","execution_count":null,"outputs":[]},{"metadata":{"id":"H7dnbkcwQphW","outputId":"107c685a-14b9-4c04-b71e-471821542702","trusted":true},"cell_type":"code","source":"p = os.getcwd()\np","execution_count":null,"outputs":[]},{"metadata":{"id":"SLludeeuQeYx","trusted":true},"cell_type":"code","source":"base_path = '/kaggle/input/improvedv2/improved'\n\ntrain_path =  '/kaggle/input/improvedv2/improved/annotation.txt' # Training data (annotation file)\ntest_path =  '/kaggle/input/improvedv2/improved/test_annotation.txt' # Training data (annotation file)\nnum_rois = 4 # Number of RoIs to process at once.\n\n# Augmentation flag\nhorizontal_flips = True # Augment with horizontal flips in training. \nvertical_flips = True   # Augment with vertical flips in training. \nrot_90 = True           # Augment with 90 degree rotations in training. \n\noutput_weight_path = os.path.join('model_frcnn_vgg.hdf5')\n\nrecord_path = os.path.join('record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n\nbase_weight_path = os.path.join(base_path, 'models/model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n\nconfig_output_filename = os.path.join('model_vgg_config.pickle')","execution_count":null,"outputs":[]},{"metadata":{"id":"dqBrd6OqRrzQ","trusted":true},"cell_type":"code","source":"# Create the config\nC = Config()\n\nC.use_horizontal_flips = horizontal_flips\nC.use_vertical_flips = vertical_flips\nC.rot_90 = rot_90\n\nC.record_path = record_path\nC.model_path = output_weight_path\nC.num_rois = num_rois\n\nC.base_net_weights = base_weight_path","execution_count":null,"outputs":[]},{"metadata":{"id":"P-JWc3Cg4nt1","outputId":"d9bf6b59-c214-4ab0-89f7-65da13dec50c","trusted":true},"cell_type":"code","source":"C.base_net_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=keras.applications.vgg16.VGG16(weights='imagenet')\nmodel.save_weights(C.model_path)","execution_count":null,"outputs":[]},{"metadata":{"id":"pQ4rzBO4RsiD","outputId":"09a3cf7b-18a7-442b-99fc-56e4cd5a232b","trusted":true},"cell_type":"code","source":"#--------------------------------------------------------#\n# This step will spend some time to load the data        #\n#--------------------------------------------------------#\nst = time.time()\ntrain_imgs, classes_count, class_mapping = get_data(train_path)\nprint()\nprint('Spend %0.2f mins to load the data' % ((time.time()-st)/60) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_mapping","execution_count":null,"outputs":[]},{"metadata":{"id":"P0GIxRgORwlu","outputId":"b77c86c2-0a93-4759-ac4e-096dd61c7ffc","trusted":true},"cell_type":"code","source":"if 'bg' not in classes_count:\n\tclasses_count['bg'] = 0\n\tclass_mapping['bg'] = len(class_mapping)\n# e.g.\n#    classes_count: {'usask_1': 5807,'arvalis_1': 45716,'inrae_1': 3701,'ethz_1': 51489,'arvalis_3': 16665,'rres_1': 9635,'bg':0}\n#    class_mapping: {'usask_1': 0,'arvalis_1': 1,'inrae_1': 2,'ethz_1': 3,'arvalis_3': 4,'rres_1': 5,'bg': 6}\nC.class_mapping = class_mapping\n\nprint('Training images per class:')\npprint.pprint(classes_count)\nprint('Num classes (including bg) = {}'.format(len(classes_count)))\nprint(class_mapping)\n\n# Save the configuration\nwith open(config_output_filename, 'wb') as config_f:\n\tpickle.dump(C,config_f)\n\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vIRZoHCZrFPl","outputId":"4f640b07-c213-4b06-cc5b-f96b8471a916","trusted":true},"cell_type":"code","source":"# Shuffle the images with seed\nrandom.seed(1)\nrandom.shuffle(train_imgs)\n\nprint('Num train samples (images) {}'.format(len(train_imgs)))","execution_count":null,"outputs":[]},{"metadata":{"id":"lpKVm6D1rTb6","trusted":true},"cell_type":"code","source":"# Get train data generator which generate X, Y, image_data\ndata_gen_train = get_anchor_gt(train_imgs, C, img_length_calc_function, mode='train')","execution_count":null,"outputs":[]},{"metadata":{"id":"ymkExpDJrjqu"},"cell_type":"markdown","source":"#### Explore 'data_gen_train'\n\ndata_gen_train is an **generator**, so we get the data by calling **next(data_gen_train)**"},{"metadata":{"id":"IrFf1aW0rcty","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#X, Y, image_data = next(data_gen_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"NiDLPIkkr3I9"},"cell_type":"markdown","source":"Build the Model"},{"metadata":{"id":"be75GVAEruRm","trusted":true},"cell_type":"code","source":"input_shape_img = (None, None, 3)\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(None, 4))\n\n# define the base network (VGG here, can be Resnet50, Inception, etc)\nshared_layers = nn_base(img_input, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])","execution_count":null,"outputs":[]},{"metadata":{"id":"da63dP-csIIz","outputId":"7db44b46-4d65-4a66-cc18-0246d588f399","trusted":true},"cell_type":"code","source":"# define the RPN, built on the base layers\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\nrpn = rpn_layer(shared_layers, num_anchors)\n\nclassifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n\nmodel_rpn = Model(img_input, rpn[:2])\nmodel_classifier = Model([img_input, roi_input], classifier)\n\n# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\nmodel_all = Model([img_input, roi_input], rpn[:2] + classifier)\n\n# Because the google colab can only run the session several hours one time (then you need to connect again), \n# we need to save the model and load the model to continue training\nif not os.path.isfile(C.model_path):\n    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n    try:\n        print('This is the first time of your training')\n        print('loading weights from {}'.format(C.base_net_weights))\n        model_rpn.load_weights(C.base_net_weights, by_name=True)\n        model_classifier.load_weights(C.base_net_weights, by_name=True)\n    except:\n        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n            https://github.com/fchollet/keras/tree/master/keras/applications')\n    \n    # Create the record.csv file to record losses, acc and mAP\n    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\nelse:\n    # If this is a continued training, load the trained model from before\n    print('Continue training based on previous trained model')\n    print('Loading weights from {}'.format(C.model_path))\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\n    \n    # Load the records\n    '''record_df = pd.read_csv(record_path)\n\n    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n    r_class_acc = record_df['class_acc']\n    r_loss_rpn_cls = record_df['loss_rpn_cls']\n    r_loss_rpn_regr = record_df['loss_rpn_regr']\n    r_loss_class_cls = record_df['loss_class_cls']\n    r_loss_class_regr = record_df['loss_class_regr']\n    r_curr_loss = record_df['curr_loss']\n    r_elapsed_time = record_df['elapsed_time']\n    r_mAP = record_df['mAP']\n\n    print('Already train %dK batches'% (len(record_df)))'''","execution_count":null,"outputs":[]},{"metadata":{"id":"KyW8RrCksOFa","trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\noptimizer_classifier = Adam(lr=1e-5)\nmodel_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\nmodel_all.compile(optimizer='sgd', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"id":"xgfKMiQHBcoA","trusted":true},"cell_type":"code","source":"# Training setting\ntotal_epochs = len(record_df)\nr_epochs = len(record_df)\n\nepoch_length = 360\nnum_epochs = 2   #Just of sharing the karnel running with 2 epoch , you try with min 20 epochs\niter_num = 0\n\ntotal_epochs += num_epochs\n\nlosses = np.zeros((epoch_length, 5))\nrpn_accuracy_rpn_monitor = []\nrpn_accuracy_for_epoch = []\n\nif len(record_df)==0:\n    best_loss = np.Inf\nelse:\n    best_loss = np.min(r_curr_loss)","execution_count":null,"outputs":[]},{"metadata":{"id":"EYQ3tEolBmdl","outputId":"b14ae236-52df-4da9-ce0f-e6ff5b96c37a","trusted":true},"cell_type":"code","source":"start_time = time.time()\nfor epoch_num in range(num_epochs):\n\n    progbar = generic_utils.Progbar(epoch_length)\n    print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n    \n    r_epochs += 1\n\n    while True:\n        if 1:\n\n            if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n                rpn_accuracy_rpn_monitor = []\n#                 print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n                if mean_overlapping_bboxes == 0:\n                    print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n\n            # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n            X, Y, img_data = next(data_gen_train)\n            # Train rpn model and get loss value [_, loss_rpn_cls, loss_rpn_regr]\n            loss_rpn = model_rpn.train_on_batch(X, Y)\n            #print(loss_rpn)\n            # Get predicted rpn from rpn model [rpn_cls, rpn_regr]\n            P_rpn = model_rpn.predict_on_batch(X)\n            # R: bboxes (shape=(300,4))\n            # Convert rpn layer to roi bboxes\n            R = rpn_to_roi(P_rpn[0], P_rpn[1], C, K.common.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n            # X2: bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n            # Y1: one hot code for bboxes from above => x_roi (X)\n            # Y2: corresponding labels and corresponding gt bboxes\n            X2, Y1, Y2, IouS = calc_iou(R, img_data, C, class_mapping)\n            # If X2 is None means there are no matching bboxes\n            if X2 is None:\n                rpn_accuracy_rpn_monitor.append(0)\n                rpn_accuracy_for_epoch.append(0)\n                continue\n            \n            # Find out the positive anchors and negative anchors\n            neg_samples = np.where(Y1[0, :, -1] == 1)\n            pos_samples = np.where(Y1[0, :, -1] == 0)\n\n            if len(neg_samples) > 0:\n                neg_samples = neg_samples[0]\n            else:\n                neg_samples = []\n\n            if len(pos_samples) > 0:\n                pos_samples = pos_samples[0]\n            else:\n                pos_samples = []\n\n            rpn_accuracy_rpn_monitor.append(len(pos_samples))\n            rpn_accuracy_for_epoch.append((len(pos_samples)))\n\n            if C.num_rois > 1:\n                # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n                if len(pos_samples) < C.num_rois//2:\n                    selected_pos_samples = pos_samples.tolist()\n                else:\n                    selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n                \n                # Randomly choose (num_rois - num_pos) neg samples\n                try:\n                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n                except:\n                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n                \n                # Save all the pos and neg samples in sel_samples\n                sel_samples = selected_pos_samples + selected_neg_samples\n            else:\n                # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n                selected_pos_samples = pos_samples.tolist()\n                selected_neg_samples = neg_samples.tolist()\n                if np.random.randint(0, 2):\n                    sel_samples = random.choice(neg_samples)\n                else:\n                    sel_samples = random.choice(pos_samples)\n\n            # training_data: [X, X2[:, sel_samples, :]]\n            # labels: [Y1[:, sel_samples, :], Y2[:, sel_samples, :]]\n            #  X                     => img_data resized image\n            #  X2[:, sel_samples, :] => num_rois (4 in here) bboxes which contains selected neg and pos\n            #  Y1[:, sel_samples, :] => one hot encode for num_rois bboxes which contains selected neg and pos\n            #  Y2[:, sel_samples, :] => labels and gt bboxes for num_rois bboxes which contains selected neg and pos\n            loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n            losses[iter_num, 0] = loss_rpn[1]\n            losses[iter_num, 1] = loss_rpn[2]\n\n            losses[iter_num, 2] = loss_class[1]\n            losses[iter_num, 3] = loss_class[2]\n            losses[iter_num, 4] = loss_class[3]\n\n            iter_num += 1\n\n            progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n                                      ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3]))])\n\n            if iter_num == epoch_length:\n                loss_rpn_cls = np.mean(losses[:, 0])\n                loss_rpn_regr = np.mean(losses[:, 1])\n                loss_class_cls = np.mean(losses[:, 2])\n                loss_class_regr = np.mean(losses[:, 3])\n                class_acc = np.mean(losses[:, 4])\n\n                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n                rpn_accuracy_for_epoch = []\n\n                if C.verbose:\n                    print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n                    print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n                    print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n                    print('Loss RPN regression: {}'.format(loss_rpn_regr))\n                    print('Loss Detector classifier: {}'.format(loss_class_cls))\n                    print('Loss Detector regression: {}'.format(loss_class_regr))\n                    print('Total loss: {}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n                    print('Elapsed time: {}'.format(time.time() - start_time))\n                    elapsed_time = (time.time()-start_time)/60\n\n                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n                iter_num = 0\n                start_time = time.time()\n\n                if curr_loss < best_loss:\n                    if C.verbose:\n                        print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n                    best_loss = curr_loss\n                    model_all.save_weights(C.model_path)\n\n                new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n                           'class_acc':round(class_acc, 3), \n                           'loss_rpn_cls':round(loss_rpn_cls, 3), \n                           'loss_rpn_regr':round(loss_rpn_regr, 3), \n                           'loss_class_cls':round(loss_class_cls, 3), \n                           'loss_class_regr':round(loss_class_regr, 3), \n                           'curr_loss':round(curr_loss, 3), \n                           'elapsed_time':round(elapsed_time, 3), \n                           'mAP': 0}\n\n                record_df = record_df.append(new_row, ignore_index=True)\n                record_df.to_csv(record_path, index=0)\n\n                break\n\n        else: \n            #Exception as e:\n            print('Exception: {}'.format(e))\n            continue\n\nprint('Training complete, exiting.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TEST ****"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef format_img_size(img, C):\n    \"\"\" formats the image size based on config \"\"\"\n    img_min_side = float(C.im_size)\n    (height, width, _) = img.shape\n\n    if width <= height:\n        ratio = img_min_side/width\n        new_height = int(ratio * height)\n        new_width = int(img_min_side)\n    else:\n        ratio = img_min_side/height\n        new_width = int(ratio * width)\n        new_height = int(img_min_side)\n    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n    fx = width / float(new_width)\n    fy = height / float(new_height)\n    return img, ratio, fx, fy\n\n\ndef format_img_channels(img, C):\n    \"\"\" formats the image channels based on config \"\"\"\n    img = img[:, :, (2, 1, 0)]\n    img = img.astype(np.float32)\n    img[:, :, 0] -= C.img_channel_mean[0]\n    img[:, :, 1] -= C.img_channel_mean[1]\n    img[:, :, 2] -= C.img_channel_mean[2]\n    img /= C.img_scaling_factor\n    img = np.transpose(img, (2, 0, 1))\n    img = np.expand_dims(img, axis=0)\n    return img\n\n\ndef format_img(img, C):\n    \"\"\" formats an image for model prediction based on config \"\"\"\n    img, ratio, fx, fy = format_img_size(img, C)\n    img = format_img_channels(img, C)\n    return img, ratio, fx, fy\n\n\ndef get_real_coordinates(ratio, x1, y1, x2, y2):\n    \"\"\" Method to transform the coordinates of the bounding box to its original size \"\"\"\n    real_x1 = int(round(x1 // ratio))\n    real_y1 = int(round(y1 // ratio))\n    real_x2 = int(round(x2 // ratio))\n    real_y2 = int(round(y2 // ratio))\n\n    return (real_x1, real_y1, real_x2 ,real_y2)\n\n\ndef get_map(pred, gt, f):\n    T = {}\n    P = {}\n    fx, fy = f\n\n    for bbox in gt:\n        bbox['bbox_matched'] = False\n\n    pred_probs = np.array([s['prob'] for s in pred])\n    box_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]\n\n    for box_idx in box_idx_sorted_by_prob:\n        pred_box = pred[box_idx]\n        pred_class = pred_box['class']\n        pred_x1 = pred_box['x1']\n        pred_x2 = pred_box['x2']\n        pred_y1 = pred_box['y1']\n        pred_y2 = pred_box['y2']\n        pred_prob = pred_box['prob']\n        if pred_class not in P:\n            P[pred_class] = []\n            T[pred_class] = []\n        P[pred_class].append(pred_prob)\n        found_match = False\n\n        for gt_box in gt:\n            gt_class = gt_box['class']\n            gt_x1 = gt_box['x1']/fx\n            gt_x2 = gt_box['x2']/fx\n            gt_y1 = gt_box['y1']/fy\n            gt_y2 = gt_box['y2']/fy\n            gt_seen = gt_box['bbox_matched']\n            if gt_class != pred_class:\n                continue\n            if gt_seen:\n                continue\n            iou = data_generators.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n            if iou >= 0.5:\n                found_match = True\n                gt_box['bbox_matched'] = True\n                break\n            else:\n                continue\n\n        T[pred_class].append(int(found_match))\n\n    for gt_box in gt:\n        if not gt_box['bbox_matched']:\n            if gt_box['class'] not in P:\n                P[gt_box['class']] = []\n                T[gt_box['class']] = []\n\n            T[gt_box['class']].append(1)\n            P[gt_box['class']].append(0)\n    return T, P\n\n\nif __name__ == \"__main__\":\n\n\n\n    # turn off any data augmentation at test time\n    C.use_horizontal_flips = False\n    C.use_vertical_flips = False\n    C.rot_90 = False\n\n    class_mapping = C.class_mapping\n    if 'bg' not in class_mapping:\n        class_mapping['bg'] = len(class_mapping)\n\n    class_mapping = {v: k for k, v in class_mapping.items()}\n    print(class_mapping)\n    class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\n    \n    input_shape_img = (None, None, 3)\n    img_input = Input(shape=input_shape_img)\n    roi_input = Input(shape=(None, 4))\n    feature_map_input = Input(shape=(None,None,1))\n\n    # define the base network (VGG here, can be Resnet50, Inception, etc)\n    shared_layers = nn_base(img_input, trainable=True)\n    # define the RPN, built on the base layers\n    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\n    rpn_layers = rpn_layer(shared_layers, num_anchors)\n\n    classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping))\n\n    model_rpn = Model(img_input, rpn_layers)\n    model_classifier_only = Model([feature_map_input, roi_input], classifier)\n\n    model_classifier = Model([feature_map_input, roi_input], classifier)\n\n    print('Loading weights from {}'.format(C.model_path))       # model_path specified in config file\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\n\n    model_rpn.compile(optimizer='sgd', loss='mse')\n    model_classifier.compile(optimizer='sgd', loss='mse')\n\n    all_imgs = []\n    classes = {}\n    all_imgs, _, _ = get_data(test_path)\n    test_imgs = [s for s in all_imgs if s['imageset'] == 'train']      # mAP\n    print(\"DEBUGGING 218: test imgs: \", len(test_imgs))\n    classification_threshold = 0.8\t\t# threshold above which we classify as positive\n\n    # for mAP\n    T, P = {}, {}\n\n    counter = 0\n    for idx, img_data in enumerate(test_imgs):\n        print('{}/{}'.format(idx, len(test_imgs)))\n        img_name = img_data['filepath'].split('/')[-1]\n        print(\"DEBUGGING 232 img_name:\", img_name)\n        print(\"img {}: {}\".format(str(counter), img_name))\n        counter += 1\n        start_time = time.time()\n        img_path = '/kaggle/input/improvedv2/improved/resizetrain'\n        filepath = os.path.join(img_path, img_name)\n        print(\"DEBUGGING filepath:\", filepath)\n        img = io.imread(filepath)\n\n        img ,ratio, fx, fy = format_img(img, C)\n        img = np.reshape(img,(1,448,448,3))\n        #X = np.transpose(X, (0, 2, 3, 1))\n\n        # get the feature maps and output from the RPN\n        [Y1, Y2,F] = model_rpn.predict(img)\n        # R = bboxes (300 ,4)\n        R = rpn_to_roi(Y1, Y2, C,K.common.image_dim_ordering(), overlap_thresh=0.7)\n\n        # convert from (x1,y1,x2,y2) to (x,y,w,h)\n        R[:, 2] -= R[:, 0]\n        R[:, 3] -= R[:, 1]\n\n        # apply the spatial pyramid pooling to the proposed regions\n        bboxes = {}\n        probs = {}\n\n        for jk in range(R.shape[0]//C.num_rois + 1):    \t# R.shape[0] = 300\n            ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n            if ROIs.shape[1] == 0:\n                break\n\n            if jk == R.shape[0]//C.num_rois:\n                # pad R\n                curr_shape = ROIs.shape\n                target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n                ROIs_padded[:, :curr_shape[1], :] = ROIs\n                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n                ROIs = ROIs_padded\n\n            # [pred_cls, pred_regr] = model_classifier_only.predict([F, ROIs])\n            [pred_cls, pred_regr] = model_classifier.predict([F, ROIs])\n\n            for ii in range(pred_cls.shape[1]):\n\n                if np.max(pred_cls[0, ii, :]) < classification_threshold or np.argmax(pred_cls[0, ii, :]) == (pred_cls.shape[2] - 1):\n                    pass\n\n                cls_name = class_mapping[np.argmax(pred_cls[0, ii, :])]\n\n                if cls_name not in bboxes:\n                    bboxes[cls_name] = []\n                    probs[cls_name] = []\n\n                (x, y, w, h) = ROIs[0, ii, :]\n\n                cls_num = np.argmax(pred_cls[0, ii, :])\t # index of predicted class\n                try:\n                    (tx, ty, tw, th) = pred_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n                    tx /= C.classifier_regr_std[0]\n                    ty /= C.classifier_regr_std[1]\n                    tw /= C.classifier_regr_std[2]\n                    th /= C.classifier_regr_std[3]\n                    x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n                except:\n                    pass\n                bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n                probs[cls_name].append(np.max(pred_cls[0, ii, :]))\n\n        #bboxes.pop('bg')    # added to avoid plotting background bbox\n        #probs.pop('bg')     # added to avoid plotting background bbox\n\n        pred_bboxs = []\n\n        for key in bboxes:\n            bbox = np.array(bboxes[key])\n\n            new_boxes, new_probs = improved_non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n            for jk in range(new_boxes.shape[0]):\n                (x1, y1, x2, y2) = new_boxes[jk, :]\n\n                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n\n                cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n\n                textLabel = '{}: {}'.format(key, int(100*new_probs[jk]))\n                pred_bboxs.append({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]})\n\n                (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n                textOrg = (real_x1, real_y1-0)\n\n                cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n\n        print('Elapsed time = {}'.format(time.time() - start_time))\n        print(\"pred_bboxs:\", pred_bboxs)\n        #print(\"gt boxes:\", img_data['bboxes'])\n        cv2.imwrite('./results_imgs/{}'.format(img_name), img)\n        img = np.reshape(img,(448,448,3))\n        plt.imshow(img)\n        break\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}